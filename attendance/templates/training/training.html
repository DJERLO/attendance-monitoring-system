{% load static %}
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <link rel="icon" type="image/x-icon" href="{% static 'img/logo.png' %}">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="{% static 'css/bootstrap.min.css' %}" rel="stylesheet">
    <link href="{% static 'css/attendance.css' %}" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/sweetalert2@11/dist/sweetalert2.min.css">
    <title>Online Training</title>
</head>
<body class="bg bg-dark">
    <div class="loader-container" id="loader">
        <div class="ripple"></div>
    </div>
    <main>
        <div class="container text-center">
            <div class="row g-0"> 
                <div class="col-md-12" id="video-container">
                    <canvas class="align-self-stretch" id="canvas"></canvas>
                    <video class="align-self-stretch" id="video" autoplay muted></video>
                </div>
            </div>
        </div>
    </main>

    <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11/dist/sweetalert2.all.min.js"></script>
    <script defer src="{% static 'js/bootstrap.bundle.min.js' %}"></script>
    <script defer src="{% static 'js/face-api.min.js' %}"></script>
    <script type="module">
        import tensorflowtfjs from 'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.21.0/+esm'
    </script>   
    
    <script>
        document.addEventListener('DOMContentLoaded', async () => {
            const loader = document.getElementById('loader');
            const video = document.getElementById('video');
            const canvas = document.getElementById('canvas');
            const context = canvas.getContext('2d');
            const form = document.getElementById('form-container');
            
            let lastDetectionTime = Date.now();
            const detectionInterval = 3000; // 2 seconds between consecutive face captures
            let matches = [];        // Array to hold recognized names
            let employee_numbers = [];  // hold recognized employee_ids
            let messages = [];       // hold recognized messages

            let employeeName = document.getElementById('employeeName');
            let employeeID = document.getElementById('employeeID');


            function adjustCanvasSize(video, canvas) {
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
                video.width = video.videoWidth;
                video.height = video.videoHeight;
            }

            function getCameraStream() {
                const constraints = {
                    video: {
                        width: { ideal: window.innerWidth },
                        height: { ideal: window.innerHeight },
                        facingMode: "user"
                    }
                };

                // Access webcam stream
                navigator.mediaDevices.getUserMedia(constraints)
                .then(stream => {
                    video.srcObject = stream;
                    video.addEventListener('loadedmetadata', () => {
                        adjustCanvasSize(video, canvas);
                    });
                })
                .catch(err => {
                    console.error("Error accessing webcam: " + err);
                    alert("Could not access the camera. Please check camera permissions and try again.");
                });
            }

            async function loadModels() {
                try {
                    const modelPath = '{% static "js/" %}';
                    await faceapi.nets.ssdMobilenetv1.loadFromUri(modelPath);
                    await faceapi.nets.tinyFaceDetector.loadFromUri(modelPath);
                    await faceapi.nets.faceLandmark68Net.loadFromUri(modelPath);
                    await faceapi.nets.faceRecognitionNet.loadFromUri(modelPath);
                } catch (error) {
                    console.error("Error loading models:", error);
                }
            }

            async function detectAndCaptureFaces() {
                console.log(messages)
                isFaceCloseEnough = true
                const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
                    .withFaceLandmarks()
                    .withFaceDescriptors();

                context.clearRect(0, 0, canvas.width, canvas.height);
                let faceNames = [];

                detections.forEach((detection, index) => {
                    const box = detection.detection.box;
                    const faceSize = box.width * box.height;
                    const canvasSize = canvas.width * canvas.height;
                    var name = "";  
                    const employeeID = employee_numbers[index] || "";
                    
                    if (faceSize < canvasSize * 0.1) {
                        // Face is too small, prompt user to move closer
                        context.font = '20px Arial';
                        context.fillStyle = 'white'; // Text color

                        // Calculate the dimensions of the text
                        const text = 'Please move closer to the camera';
                        const textWidth = context.measureText(text).width;
                        const textHeight = 20; // Approximate height based on font size

                        // Padding for the rectangle
                        const messagepadding = 10;

                        // Calculate rectangle position and dimensions
                        const rectX = (canvas.width - textWidth) / 2 - messagepadding; // Center the rectangle
                        const rectY = (canvas.height - textHeight) / 2 - messagepadding; // Center vertically
                        const rectWidth = textWidth + messagepadding * 2; // Width of rectangle
                        const rectHeight = textHeight + messagepadding * 2; // Height of rectangle

                        // Draw the background rectangle
                        context.fillStyle = 'rgba(255, 0, 0, 0.7)'; // Semi-transparent red
                        context.fillRect(rectX, rectY, rectWidth, rectHeight);

                        // Draw the text on top of the rectangle
                        context.fillStyle = 'white'; // Set text color to white
                        context.fillText(text, (canvas.width / 2) - (textWidth / 2), (canvas.height / 2) + (textHeight / 4));
                        
                        //Set isFaceCloseEnough to false so it won't send the draw an image on the canvas
                        isFaceCloseEnough = false
                        name = messages[index];
                        faceNames.push(name);

                        // Draw the rectangle around the face
                        context.beginPath();
                        context.rect(box.x, box.y, box.width, box.height);
                        context.lineWidth = 5;
                        context.strokeStyle = 'red';
                        context.stroke();

                        // Set the fill style for the text background
                        context.fillStyle = 'rgba(255, 0, 0, 0.7)'; // Semi-transparent red
                        const padding = 10; // Padding around the text

                        // Draw the background rectangle for the text
                        context.fillRect(box.x, box.y > 10 ? box.y - 30 : 10, context.measureText('Move Closer').width + padding * 2, 30);

                        // Set text properties
                        context.fillStyle = 'white'; // Text color
                        context.font = '20px Arial';
                        context.fillText('Move Closer', box.x + padding, box.y > 10 ? box.y - 10 : 20); // Adjust position with padding  
                    }
                    
                    else{
                        // Determine if the face is recognized or not
                        name = messages[index];
                        faceNames.push(name);

                        // Draw bounding box and label
                        context.beginPath();
                        context.rect(box.x, box.y, box.width, box.height);
                        context.lineWidth = 5;
                        context.strokeStyle = (name === "Fake") ? 'red' : 
                        (name === undefined) ? 'yellow' : 'green';
                        context.stroke();

                        context.font = '20px Arial';
                        context.fillStyle = (name === "Fake") ? 'red' : 
                        (name === undefined) ? 'yellow' : 'green';
                        context.fillText(name, box.x, box.y > 10 ? box.y - 5 : 10);  
                    }

                    
                });
                //Check if the face is close enough and there is a detection of the face
                if (isFaceCloseEnough && detections.length > 0 && (Date.now() - lastDetectionTime > detectionInterval)) {
                    lastDetectionTime = Date.now(); 
                    await captureAndSendImage(faceNames); 
                } else if (detections.length === 0 && (Date.now() - lastDetectionTime > detectionInterval)) {
                    console.log('No faces detected.');
                }

                requestAnimationFrame(detectAndCaptureFaces);
            }

            async function captureAndSendImage(faceNames) {
                console.log("Matching...");
                context.drawImage(video, 0, 0, canvas.width, canvas.height);
                const imgData = canvas.toDataURL('image/jpeg');
                
                fetch('/online-training/', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                        'X-CSRFToken': '{{ csrf_token }}',
                    },
                    body: JSON.stringify({ image: imgData }),
                })
                .then(response => response.json())
                .then(data => {
                    console.log('Server response:', data);
                    
                    
                    // Check if 'results' exists and has entries
                    if (data.results && data.results.length > 0) {
                        // Loop through each result
                        messages = new Array(data.results.length);
                        data.results.forEach((result, index) => {
                            const { class_index, confidence, message, coordinates } = result;

                            // Store the message in the messages array at the current index
                            messages[index] = message; 

                        });
                    } else if (data.error) {
                        console.error('Error:', data.error); // Log any error message returned from the server
                    } else {
                        console.log('No faces detected.');
                    }
                
                })
                .catch((error) => {
                    console.error('Error:', error);
                });
            }

            await loadModels();
            getCameraStream();
    
            window.onresize = function() {
                video.pause();
                getCameraStream();
                adjustCanvasSize(video, canvas);
            };
    
            video.addEventListener('loadedmetadata', () => {
                detectAndCaptureFaces();
            });

            setTimeout(() => {
                loader.style.display = 'none'; // Hide the spinner after 5 seconds
            }, 6000);
        });
    </script>    
</body>
</html>
