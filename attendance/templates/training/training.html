{% load static %}
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="{% static 'css/bootstrap.min.css' %}" rel="stylesheet">
    <link href="{% static 'css/attendance.css' %}" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/sweetalert2@11/dist/sweetalert2.min.css">
    <title>Online Training</title>
</head>
<body class="bg bg-dark">
    <div class="loader-container" id="loader">
        <div class="ripple"></div>
    </div>
    <main>
        <div class="container text-center">
            <div class="row g-0"> 
                <div class="col-md-12" id="video-container">
                    <canvas class="align-self-stretch" id="canvas"></canvas>
                    <video class="align-self-stretch" id="video" autoplay muted></video>
                </div>
            </div>
        </div>
    </main>

    <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11/dist/sweetalert2.all.min.js"></script>
    <script defer src="{% static 'js/bootstrap.bundle.min.js' %}"></script>
    <script defer src="{% static 'js/face-api.min.js' %}"></script>
    <script type="module">
        import tensorflowtfjs from 'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.21.0/+esm'
    </script>   
    
    <script>
        document.addEventListener('DOMContentLoaded', async () => {
            const loader = document.getElementById('loader');
            const video = document.getElementById('video');
            const canvas = document.getElementById('canvas');
            const context = canvas.getContext('2d');
            const form = document.getElementById('form-container');
            
            let lastDetectionTime = Date.now();
            const detectionInterval = 500; // 2 seconds between consecutive face captures
            let matches = [];        // Array to hold recognized names
            let employee_numbers = [];  // hold recognized employee_ids
            let messages = [];       // hold recognized messages

            let employeeName = document.getElementById('employeeName');
            let employeeID = document.getElementById('employeeID');


            function adjustCanvasSize(video, canvas) {
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
                video.width = video.videoWidth;
                video.height = video.videoHeight;
            }

            function getCameraStream() {
                const constraints = {
                    video: {
                        width: { ideal: window.innerWidth },
                        height: { ideal: window.innerHeight },
                        facingMode: "user"
                    }
                };

                // Access webcam stream
                navigator.mediaDevices.getUserMedia(constraints)
                .then(stream => {
                    video.srcObject = stream;
                    video.addEventListener('loadedmetadata', () => {
                        adjustCanvasSize(video, canvas);
                    });
                })
                .catch(err => {
                    console.error("Error accessing webcam: " + err);
                    alert("Could not access the camera. Please check camera permissions and try again.");
                });
            }

            async function loadModels() {
                try {
                    const modelPath = '{% static "js/" %}';
                    await faceapi.nets.ssdMobilenetv1.loadFromUri(modelPath);
                    await faceapi.nets.tinyFaceDetector.loadFromUri(modelPath);
                    await faceapi.nets.faceLandmark68Net.loadFromUri(modelPath);
                    await faceapi.nets.faceRecognitionNet.loadFromUri(modelPath);
                } catch (error) {
                    console.error("Error loading models:", error);
                }
            }

            async function detectAndCaptureFaces() {
                console.log(messages)
                const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
                    .withFaceLandmarks()
                    .withFaceDescriptors();

                context.clearRect(0, 0, canvas.width, canvas.height);
                let faceNames = [];

                detections.forEach((detection, index) => {
                    const box = detection.detection.box;
                    // Determine if the face is recognized or not
                    const name = messages[index] || "Unknown";  
                    const employeeID = employee_numbers[index] || "N/A";
                    faceNames.push(name);

                    // Draw bounding box and label
                    context.beginPath();
                    context.rect(box.x, box.y, box.width, box.height);
                    context.lineWidth = 5;
                    context.strokeStyle = (name === "Fake") ? 'red' : 
                      (name === "Uncertain") ? 'yellow' : 'green';
                    context.stroke();

                    context.font = '20px Arial';
                    context.fillStyle = (name === "Fake") ? 'red' : 
                      (name === "Uncertain") ? 'yellow' : 'green';
                    context.fillText(name, box.x, box.y > 10 ? box.y - 5 : 10);  
                });

                if (detections.length > 0 && (Date.now() - lastDetectionTime > detectionInterval)) {
                    lastDetectionTime = Date.now(); 
                    await captureAndSendImage(faceNames); 
                } else if (detections.length === 0 && (Date.now() - lastDetectionTime > detectionInterval)) {
                    console.log('No faces detected.');
                    
                }

                requestAnimationFrame(detectAndCaptureFaces);
            }

            async function captureAndSendImage(faceNames) {
                console.log("Matching...");
                context.drawImage(video, 0, 0, canvas.width, canvas.height);
                const imgData = canvas.toDataURL('image/jpeg');
                
                fetch('/online-training/', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                        'X-CSRFToken': '{{ csrf_token }}',
                    },
                    body: JSON.stringify({ image: imgData }),
                })
                .then(response => response.json())
                .then(data => {
                    console.log('Server response:', data);
                    const { class_index, confidence, message } = data.result;
                    messages = [message]; // Set the messages array to contain only the latest message
                
                })
                .catch((error) => {
                    console.error('Error:', error);
                });
            }

            await loadModels();
            getCameraStream();
    
            window.onresize = function() {
                video.pause();
                getCameraStream();
                adjustCanvasSize(video, canvas);
            };
    
            video.addEventListener('loadedmetadata', () => {
                detectAndCaptureFaces();
            });

            setTimeout(() => {
                loader.style.display = 'none'; // Hide the spinner after 5 seconds
            }, 6000);
        });
    </script>    
</body>
</html>
